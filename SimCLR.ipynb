{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaCamilloni/dd2412-deep-learning-advanced-fixmatch/blob/main/SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmklZXemhspB"
      },
      "source": [
        "# SimCLR\n",
        "\n",
        "In this practial exercise, you are going to implement the famous SimCLR [1] algorithm for self-supervised learning. While not that old, it is often seen as the 'classic' approach to contrastive learning and it is still used as a baseline against which newer approaches are compared. Most of the more recent algorithms are just slight alterations of this approach, so knowing how it works will pay off in the future. Due to the growing amounts of data, self-supervised learning is likely to become more and more relevant in the future. \n",
        "\n",
        "Or as Yan LeCun and Ishan Misra put it: SSL may be helpful to unlock the [dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/).\n",
        "\n",
        "## Getting labels is expensive and can be ethically difficult\n",
        "\n",
        "Regular supervised learning requires each data point to be labeled with ground truth information. Additionally, it requires large amounts of data. This unfortunately means that humans have to spend a lot of time hand-labeling data before we can train our networks. If determining the ground truth requires expert knowledge (e.g. diagnosing a disease based on a brain scan), getting large amounts of data is very expensive and time-consuming. In other cases, it can be very repetitive, mind-numbing work: \"Click all images containing traffic lights.\" Since nobody wants to do this kind of work, it is often outsourced to crowd-workers who earn very little and have generally bad working conditions:\n",
        "https://thegoodai.co/2021/02/03/who-is-labeling-your-data/\n",
        "\n",
        "## SSL is a form of pretraining\n",
        "\n",
        "Self-supervised learning now takes a bit of a different approach than supervised learning. It simply outsources the labeling to the computer! The computer *supervises itself*.\n",
        "\n",
        "We first pretrain our model on a so-called *pretext task* using a large **unlabeled** dataset, then fine-tune it on a smaller, labeled dataset. The pretext tasks often consist of supervised learning, but use labels that the computer can automatically generate for any image. Afterwards, we finetune our model using regular supervised learning on a smaller dataset. \n",
        "\n",
        "One example for a pretext task would be to rotate the input image and let the network predict the degree of rotation. There's no need for human labeling, since the program has the information about how it rotated the image, so it can use that as a label right away. To solve the task, the network needs to learn e.g. that the sky is usually at the top of the image and that cars have their wheels on the bottom of the image. So it is forced to learn to recognise what the image contains, which is probably useful for the downstream task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWk3Os9GrTIC"
      },
      "source": [
        "**Task** Come up with a simple pretext task yourself for any kind of dataset you like and describe it in a sentence or two. For example: \"For pretraining a dataset of dog images, I would rotate the images and let the network predicted how the image was rotated.\" But don't simply use rotation, come up with something yourself!\n",
        "\n",
        "Hint: Your program can change anything about the image and use information about that change to create a label. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14qeLONXsriG"
      },
      "source": [
        "**Answer** \n",
        "Various Types of Transformations like mirroring, flipping, changing colors spaces etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMhB8RRhhZQN"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "No need for you to implement the boring stuff. Just run the cells, we'll need it further down. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P3HrlIokhToe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce8302c-a98b-451c-8409-eacccf65497a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jax-resnet\n",
            "  Downloading jax_resnet-0.0.4-py2.py3-none-any.whl (11 kB)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 14.7 MB/s \n",
            "\u001b[?25hCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from jax-resnet) (0.3.23)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from jax-resnet) (0.3.22+cuda11.cudnn805)\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.1-py3-none-any.whl (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 65.3 MB/s \n",
            "\u001b[?25hCollecting chex>=0.0.4\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.3.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->jax-resnet) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax->jax-resnet) (1.7.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax->jax-resnet) (0.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.3)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax->jax-resnet) (5.10.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax->jax-resnet) (3.9.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax->jax-resnet) (1.0.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from flax->jax-resnet) (6.0)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax->jax-resnet) (3.2.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=11.1->flax->jax-resnet) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->jax-resnet) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->jax-resnet) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->jax-resnet) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->jax-resnet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->flax->jax-resnet) (1.15.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=6d82cd7e0a7921d5b3ff9e2b6d874ede358d7135a835cd991b7f52ac1c02866c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=9c523a68dd9352cdcfe2b5c1d5fb74e51f74b52418befc6e617c841b4b57b37c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: commonmark, chex, rich, optax, pynndescent, flax, umap-learn, jax-resnet\n",
            "Successfully installed chex-0.1.5 commonmark-0.9.1 flax-0.6.1 jax-resnet-0.0.4 optax-0.1.3 pynndescent-0.5.7 rich-12.6.0 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "%pip install jax-resnet optax umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from jax import random\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import umap\n",
        "import jax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "from typing import Any\n",
        "import tqdm\n",
        "import jax.numpy as jnp\n",
        "import jax_resnet\n",
        "from jax.config import config\n",
        "import flax.linen as nn\n",
        "from flax.core.frozen_dict import freeze, unfreeze\n",
        "config.update(\"jax_debug_nans\", True)"
      ],
      "metadata": {
        "id": "hLuiJEqAv-Ei"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ll_OnOlThl-U"
      },
      "outputs": [],
      "source": [
        "# Augmentation\n",
        "\n",
        "# Slightly changed from source: \n",
        "# https://github.com/Spijkervet/SimCLR/blob/master/simclr/modules/transformations/simclr.py\n",
        "\n",
        "class TransformsSimCLR:\n",
        "    \"\"\"\n",
        "    A stochastic data augmentation module that transforms any given data example randomly\n",
        "    resulting in two correlated views of the same example,\n",
        "    denoted x i and x j, which we consider as a positive pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_pretrain=True, is_val=False):\n",
        "        self.is_pretrain=is_pretrain\n",
        "        self.is_val=is_val\n",
        "        s = 1\n",
        "        color_jitter = torchvision.transforms.ColorJitter(\n",
        "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
        "        )\n",
        "        self.train_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
        "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                torchvision.transforms.Lambda(np.array),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.test_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Lambda(np.array),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.is_pretrain:\n",
        "          return self.train_transform(x), self.train_transform(x)\n",
        "        else:\n",
        "          if self.is_val:\n",
        "            return self.test_transform(x)\n",
        "          else:\n",
        "            return self.train_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_validation_performance(state, val_data_loader):\n",
        "  \"\"\"\n",
        "  Computes the given model's mean loss and accuracy on the validation set.\n",
        "  \"\"\"\n",
        "  val_losses = []\n",
        "  val_accs = []\n",
        "\n",
        "  val_dl_tqdm = tqdm.tqdm(val_data_loader)\n",
        "\n",
        "  for X, Y in val_dl_tqdm:\n",
        "    \n",
        "    loss, accuracy = eval_step(state, X, Y)\n",
        "    val_losses.append(loss)\n",
        "    val_accs.append(accuracy)\n",
        "\n",
        "    val_dl_tqdm.set_postfix({'val_loss': loss.item(), 'val_acc': accuracy.item()})\n",
        "\n",
        "  return jnp.array(val_losses).mean().item(), jnp.array(val_accs).mean().item()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, X, Y):\n",
        "\n",
        "    logits = state.apply_fn({'params': state.params, \n",
        "                             'batch_stats': state.batch_stats}, X,\n",
        "                              mutable=False)\n",
        "    labels = jax.nn.one_hot(Y, num_classes=10)\n",
        "    loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == Y)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "QEACTPr46DUv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3GQ21tkwS6t"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "\n",
        "# Source: https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
        "\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple,list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://colab.research.google.com/drive/1Y2IiAG69nKQSoIKAOAWC8uypdP7TGFqF?usp=sharing#scrollTo=TqDvTL_tIQCH\n",
        "def zero_grads():\n",
        "    def init_fn(_): \n",
        "        return ()\n",
        "    def update_fn(updates, state, params=None):\n",
        "        return jax.tree_map(jnp.zeros_like, updates), ()\n",
        "    return optax.GradientTransformation(init_fn, update_fn)"
      ],
      "metadata": {
        "id": "lx05jkbspj-4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yHz8UtWz9gK6"
      },
      "outputs": [],
      "source": [
        "# Training functions\n",
        "\n",
        "# Adapted from source: https://github.com/google/flax/blob/main/examples/mnist/train.py\n",
        "# Parts that use batch norm adapted from https://github.com/google/flax/blob/main/examples/imagenet/train.py\n",
        "\n",
        "@jax.jit\n",
        "def apply_model(state, X1, X2):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "  tau = 0.07\n",
        "  def loss_fn(params):\n",
        "    Z1, new_model_state = state.apply_fn({'params': params, \n",
        "                             'batch_stats': state.batch_stats}, X1,\n",
        "                              mutable=['batch_stats'])\n",
        "    Z2, new_model_state = state.apply_fn({'params': params, # We probably want to combine the two new model states, but how?\n",
        "                             'batch_stats': state.batch_stats}, X2,\n",
        "                              mutable=['batch_stats'])\n",
        "    loss = NTXent(Z1, Z2, tau)\n",
        "    return loss, (new_model_state, Z1, Z2)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, Z1, Z2)), grads = grad_fn(state.params)\n",
        "  \n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, Z1, Z2), loss\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def apply_model_pos_only(state, X1, X2):\n",
        "  \"\"\"\n",
        "  Computes gradients, loss and accuracy for a single batch,\n",
        "  uing only positive pairs.\n",
        "  \"\"\"\n",
        "  tau = 0.5\n",
        "  def loss_fn(params):\n",
        "    Z1, new_model_state = state.apply_fn({'params': params, \n",
        "                             'batch_stats': state.batch_stats}, X1,\n",
        "                              mutable=['batch_stats'])\n",
        "    Z2, new_model_state = state.apply_fn({'params': params, # TODO Amir: We probably want to combine the two new model states, but how?\n",
        "                             'batch_stats': state.batch_stats}, X2,\n",
        "                              mutable=['batch_stats'])\n",
        "    loss = PositivesOnlyLoss(Z1, Z2)\n",
        "    return loss, (new_model_state, Z1, Z2)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, Z1, Z2)), grads = grad_fn(state.params)\n",
        "  \n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, Z1, Z2), loss\n",
        "\n",
        "@jax.jit\n",
        "def apply_model_supervised(state, X, Y):\n",
        "  \"\"\"\n",
        "  Computes gradients, loss and accuracy for a single batch via supervised \n",
        "  training, not self-supervised training.\n",
        "  \"\"\"\n",
        "    \n",
        "  tau = 0.5\n",
        "  def loss_fn(params):\n",
        "    logits, new_model_state = state.apply_fn({'params': params, \n",
        "                             'batch_stats': state.batch_stats}, X,\n",
        "                              mutable=['batch_stats'])\n",
        "    labels = jax.nn.one_hot(Y, num_classes=10)\n",
        "    loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == Y)\n",
        "    return loss, (new_model_state, accuracy)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, accuracy)), grads = grad_fn(state.params)\n",
        "  \n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, accuracy), loss\n",
        "\n",
        "\n",
        "def pretrain_epoch(state, train_dl, positives_only=False):\n",
        "  \"\"\"\n",
        "  Train for a single epoch.\n",
        "  positives_only: Don't use negative pairs in contrastive training\n",
        "  \"\"\"\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  train_dl_tqdm = tqdm.tqdm(train_dl)\n",
        "\n",
        "  apply_fn = apply_model_pos_only if positives_only else apply_model\n",
        "\n",
        "  for step, ((X1,X2), Y) in enumerate(train_dl_tqdm):\n",
        "\n",
        "    (new_state, Z1, Z2), loss = apply_fn(state, X1, X2)\n",
        "    state = new_state\n",
        "    epoch_loss.append(loss)\n",
        "    train_dl_tqdm.set_postfix({'train_loss': loss.item()})\n",
        "\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  return state, train_loss\n",
        "\n",
        "def supervised_epoch(state, train_dl):\n",
        "  \"\"\"\n",
        "  Train for a single epoch with supervised loss.\n",
        "  \"\"\"\n",
        "  \n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  train_dl_tqdm = tqdm.tqdm(train_dl)\n",
        "\n",
        "  for step, (X, Y) in enumerate(train_dl_tqdm):\n",
        "\n",
        "    (new_state, accuracy), loss = apply_model_supervised(state, X, Y)\n",
        "    state = new_state\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "    train_dl_tqdm.set_postfix({'train_loss': loss.item(), 'train_acc': accuracy})\n",
        "\n",
        "  return state, jnp.array(epoch_loss).mean(), jnp.array(epoch_accuracy).mean()\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "  \"\"\"\n",
        "  Keeps track of the optimizer and updated parameters.\n",
        "  \"\"\"\n",
        "  batch_stats: Any\n",
        "\n",
        "def create_train_state(model, params, config, freeze_encoder=False):\n",
        "  if freeze_encoder:\n",
        "    tx = optax.multi_transform({'zero': zero_grads(), \n",
        "                                'sgd': optax.sgd(config[\"learning_rate\"], \n",
        "                                                  config[\"momentum\"])},\n",
        "                                freeze({'backbone':'zero', 'head': 'sgd'}))\n",
        "  else:\n",
        "    tx = optax.sgd(config[\"learning_rate\"], config[\"momentum\"])\n",
        "  return TrainState.create(\n",
        "      apply_fn=model.apply, params=params[\"params\"], tx=tx, \n",
        "      batch_stats=params[\"batch_stats\"])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-jldvhKyQZe"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We are going to use the CIFAR10 dataset. It consists of 10 classes, each represented by 6000 RGB-images of size 32x32.\n",
        "\n",
        "Have a look at a few images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pvqiAtVByLcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "50962a8b54ca495d83f43e7a7a04740e",
            "e5a3acc5bd114f25bf7ca64c63465753",
            "2667369bc5a04a7e945480ed3ca59cd0",
            "eeb7f1832ffd46789259bf1866321d95",
            "7656b60c0acd4b5aa1160341f4d3771d",
            "44549b87ef3443fb9da4606c1255f207",
            "2341c54e12c2436f9c17fc4364126737",
            "983cf5f62c1144eca6bc26a9b7c79cb7",
            "f0c51f31ed1147ecb55de0b17c89f1dd",
            "d898951063304256977589104be5a383",
            "9956ee3e7e9c40559a3e6732b59138ee"
          ]
        },
        "outputId": "cf2b021f-dd4b-4d79-fc31-711f12930bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50962a8b54ca495d83f43e7a7a04740e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "full_pretrain_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=True, is_val=False),\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "full_supervised_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=False, is_val=False),\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=False, is_val=True),\n",
        "        train=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a labeled dataset with only 1% of the labels\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "sk = StratifiedKFold(n_splits=100, shuffle=True, random_state=0)\n",
        "splits = sk.split(np.zeros(len(full_supervised_dataset)), full_supervised_dataset.targets)\n",
        "_, train_idc = list(splits)[0]\n",
        "\n",
        "supervised_dataset_1p = torch.utils.data.Subset(full_supervised_dataset, train_idc)\n",
        "\n",
        "pretrain_dataset = full_pretrain_dataset"
      ],
      "metadata": {
        "id": "eDDW-o7aojc7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P3MnHp8l0H9t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "e2e249d6-3c7e-4cfb-8a5e-b3af5cc95162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image x1 shape:  (32, 32, 3) \n",
            "Image x2 shape:  (32, 32, 3) \n",
            "class index y:  9 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fba7e530810>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(x1,x2), y = pretrain_dataset[1]\n",
        "print(\"Image x1 shape: \", x1.shape, \n",
        "      \"\\nImage x2 shape: \", x2.shape, \n",
        "      \"\\nclass index y: \", y, \"\\n\")\n",
        "\n",
        "merged_images = np.concatenate([x1,x2], axis=1)\n",
        "plt.imshow(merged_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDNcEEJR3t-l"
      },
      "source": [
        "You maybe have noticed that the transformation module we use directly gives us two images instead of one. They are always two differently augmented versions of the same source image. This will be needed for SimCLR, which teaches the network to recognise that these two images show the same object, despite the differences introduced by the augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1TV2AUu5hG_"
      },
      "source": [
        "# Contrastive learning\n",
        "\n",
        "The basic idea of contrastive learning is to learn by comparison. In the following image, the model is supposed to learn that the first two pictures show the same object, but the third one does not. Kind of like in the children's game Memory, where you try to find two matching tiles. Just... without the memory part. All the tiles are turned face up. \n",
        "\n",
        "Contrastive learning is a really stupid version of Memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ol_liYy3s8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "1e40f69a-9393-4864-ee16-080532231c83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fba7e48fc90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(x00,x01), y0 = pretrain_dataset[1]\n",
        "(x10,x11), y1 = pretrain_dataset[4]\n",
        "merged_images = np.concatenate([x00,x01,x10], axis=1)\n",
        "plt.imshow(merged_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcFqHcaO6wNs"
      },
      "source": [
        "On a mathematical level, the model outputs a feature vector (an embedding) for each input image. The embeddings of similar images (like the two images on the left) are encouraged to move closer to each other, while embeddings of dissimilar images (middle and right image) should move away from each other.\n",
        "\n",
        "This approach is fundamentally different from supervised learning. While supervised learning prescribes the exact output that the model is supposed to produce for a given input, contrastive learning only cares about the *relationship between different outputs*.\n",
        "\n",
        "Image pairs that we, the user, define as similar are called **positive pairs**, all others are **negative pairs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WZBPipS_QJK"
      },
      "source": [
        "## Similarity between images\n",
        "\n",
        "We want to be able to measure how close or far the embeddings for different images are, to see if our model can tell positive and negative pairs apart. In SimCLR, this is done using the cosine similarity. For two d-dimensional embeddings $z_1$ and $z_2$, it provides a measure for how similar they are. \n",
        "\n",
        "$cos(z_1,z_2) = \\frac{z_1}{|z_1|} \\cdot \\frac{z_2}{|z_2|}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwMAR6Rq7MBE"
      },
      "source": [
        "**Task** Implement the cosine similarity function below. It should also work if $Z_1$ and $Z_2$ are batches of shape [batch, features] and produce an output of shape [batch], containing the cosine similarities between the two feature vectors in the same row.\n",
        "\n",
        "Hint: Using for-loops here works, but is really inefficient. The efficient implementation operates on matrices, not on individual elements. It mostly looks like the equation above. If you get a problem with broadcasting between different shapes when you try to normalize $Z_1$ and $Z_2$, the function jnp.expand_dims might be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x9Y72aaMBxaD"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def cosine_similarity(Z1, Z2):\n",
        "  \"\"\"\n",
        "  Z1, Z2: Batches of image embeddings of shape [Batch,EmbeddingDim]. Z1[i,:] and\n",
        "          Z2[i,:] represent two differently augmented versions of the same image\n",
        "\n",
        "  Returns: row-wise cosine similarity between row-vectors in Z1 and Z2, \n",
        "           shape [Batch]\n",
        "  \"\"\"\n",
        "\n",
        "  Z1_norm = jnp.expand_dims(jnp.linalg.norm(Z1,axis=1),axis=1)\n",
        "  Z1_normalised = Z1/Z1_norm\n",
        "\n",
        "  Z2_norm = jnp.expand_dims(jnp.linalg.norm(Z2,axis=1),axis=1)\n",
        "  Z2_normalised = Z2/Z2_norm\n",
        "\n",
        "  cos_sims = jnp.diag(jnp.matmul(Z1_normalised,Z2_normalised.T))\n",
        "  return cos_sims\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0_flzSCp7S"
      },
      "source": [
        "Verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p3f_d3ha7qZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acd4b68-9ebf-42b0-c542-191c76de173f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Use these sample inputs to verify that your implementation is correct.\n",
        "# You can interpret Z1 and Z2 as batches of image embeddings with eight features\n",
        "# per image.\n",
        "\n",
        "random_key_0, *subkeys = random.split(random.PRNGKey(0), 3)\n",
        "\n",
        "#Batch size = 4\n",
        "#Features = 8\n",
        "Z1 = jnp.array(\n",
        "    [[0.08780523,-1.4772962,0.23108286,0.3828757,0.42218092,-1.4358605,0.17658076,1.4141593],\n",
        "    [-0.43024102,-0.2960915,0.64877045,1.0952688,0.10887499,2.5801923,-0.46863857,-0.07292866],\n",
        "    [0.8462296,-0.9795519,-0.17020774,0.5177701,-1.2235159,0.56981355,1.1847981,-1.9526129],\n",
        "    [-2.2450106,0.8794637,0.2156571,0.22987445,-0.8855504,0.180139,0.75102454,0.79618496]])\n",
        "\n",
        "Z2 = jnp.array(\n",
        "    [[0.78220856,-0.95930517,0.04278377,-0.14640434,-0.45225152,-0.164141,1.0146061,0.19397569],\n",
        "    [-0.7706246,-0.35345954,-0.67818415,-1.1203834,-0.30522528,0.669694, 1.2020552,0.87423134],\n",
        "    [0.43238065,-0.18009177,-0.13709433,-0.33463678,-1.1886245,-0.35386798,-1.0499382,0.10795221],\n",
        "    [0.23042125,-1.5269405,0.771874,-0.1904757,-1.5630287,0.8980937,-1.9551364,-0.0497684]])\n",
        "Z1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o90UHAJP8ENE"
      },
      "outputs": [],
      "source": [
        "your_output = cosine_similarity(Z1, Z2)\n",
        "desired_output = jnp.array([ 0.44447064, -0.02418898,  0.03658391, -0.19001874])\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** Wait a minute, why don't we just use the length of the difference vector $|z_1-z_2|$ instead of the cosine similarity? It's easier to compute, which could even speed up our training!"
      ],
      "metadata": {
        "id": "aAOXW2Xdq5k8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yidDdY72Azuv"
      },
      "source": [
        "**Answer** \n",
        "|z1-z2| is not appropriate to compute the similarity. We want to give high similarity to vectors having similar directions. Cosine similarity allows us to have similar images in similar directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkcxSyrE9Kkj"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "This is the point in the exercise where you need to think a bit. While the loss function is not very complex, you do need to figure out how you compute it for batched data. The paper only shows how to compute it for a single example. This is a problem that will often come up when implementing papers, so investing some time to work your brain will really pay off.\n",
        "\n",
        "If you have tried for a bit and get stuck somewhere, feel free to use the hints below that are based on where the TA needed a bit longer for the implementation than he would have liked.\n",
        "\n",
        "**Task** Implement the loss function (equation 1 in the paper) for a whole batch of image embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4PuprWGM462"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0eovqgn4_kbi"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def NTXent(Z1, Z2, tau):\n",
        "  \"\"\"\n",
        "  Z1, Z2: Batches of image embeddings of shape [Batch,EmbeddingDim]. Z1[i,:] and\n",
        "          Z2[i,:] represent two differently augmented versions of the same image\n",
        "  tau   : Temperature parameter for softmax\n",
        "\n",
        "  Returns: NTXent loss between Z1 and Z2, a single number\n",
        "  \"\"\"\n",
        "\n",
        "  N = Z1.shape[0]\n",
        "  Z1_Z2 = jnp.vstack([Z1,Z2])\n",
        "\n",
        "  Z1_Z2_norm = jnp.expand_dims(jnp.linalg.norm(Z1_Z2,axis=1),axis=1)\n",
        "  Z1Z2_normalised = Z1_Z2/Z1_Z2_norm\n",
        "\n",
        "\n",
        "  cos_sims = jnp.matmul(Z1Z2_normalised,Z1Z2_normalised.T) #2N x 2N matrix\n",
        "\n",
        "  exp_sims = jnp.exp(cos_sims/tau)\n",
        "\n",
        "  eye_invert = (jnp.eye(2*N)==0).astype(int)\n",
        "\n",
        "  denominator_sums = jnp.sum(exp_sims*eye_invert,axis=1)\n",
        "\n",
        "  L = 0\n",
        "  for i in range(N):\n",
        "    l_ij = -jnp.log(exp_sims[i,N+i]/denominator_sums[i])\n",
        "    l_ji = -jnp.log(exp_sims[N+i,i]/denominator_sums[i+N])\n",
        "    l = l_ij+l_ji\n",
        "    L += l\n",
        "\n",
        "  L = L/(2*N)\n",
        "  return L\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wm2m6xZCvS0"
      },
      "source": [
        "Verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PiKEMFQyARZT"
      },
      "outputs": [],
      "source": [
        "your_output = NTXent(Z1, Z2, 0.07)\n",
        "desired_output = 6.792835\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jDu2uOaEspK"
      },
      "source": [
        "### Hint 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on how you programmed your cosine similarity, it may be more straight-forward to just recompute the similarities here instead of using your own function."
      ],
      "metadata": {
        "id": "vaSmGkAsrqNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint 2\n"
      ],
      "metadata": {
        "id": "YEFWWqTfrunL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The denominator contains all similarities between (i,k). Only the self-similarity (i,i) is left out. The similarity between (i,j) that's in the nominator is included in the denominator."
      ],
      "metadata": {
        "id": "Cz0cu-GNrzp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint 3\n",
        "\n"
      ],
      "metadata": {
        "id": "B0zXALj9rzNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must compute the terms for both image pairs (i,j) _and_ (j,i). They will generally be different, even though on first glance they seem very similar."
      ],
      "metadata": {
        "id": "pR3_fYHsr0bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint 4"
      ],
      "metadata": {
        "id": "lVjt4SNyrz8H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TJ8xwdxGUtI"
      },
      "source": [
        "It's quite useful to have a matrix of all pairwise cosine similarities to compute everything you need. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint 5"
      ],
      "metadata": {
        "id": "VF3QqWiQkvk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on how you implemented the cosine similarity, you might be able to compute the matrix of pairwise cosine similarities with the following line: \n",
        "`cosine_similarity(jnp.expand_dims(Zs, axis=1), jnp.expand_dims(Zs, axis=0))`"
      ],
      "metadata": {
        "id": "N4GGmk4LkzNi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBoUg6L-DDOl"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "After coding the loss function, we now need a model that we can train with it!\n",
        "\n",
        "We use a simple ResNet18 as our encoder model. It ends in a global average pooling, followed by a linear layer. We discard the linear layer and stick a projection head to the end of the model instead. \n",
        "\n",
        "You could also make use of that layer instead of removing it and then attaching a new linear layer as part of the projection head, but it's a bit cleaner this way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load randomly initialized ResNet.\n",
        "\n",
        "# n_classes doesn't matter, we remove the only layer impacted by it anyway.\n",
        "# But we need to pass it to construct a ResNet. \n",
        "base_model = jax_resnet.ResNet18(n_classes=2) \n",
        "\n",
        "# Initialize the model's parameters. A dummy input is required to infer which \n",
        "# shape the parameters need to have\n",
        "random_key_1, *subkeys = random.split(random_key_0, 3)\n",
        "dummy_input = random.normal(subkeys[0],(8,32,32,3))\n",
        "base_params = base_model.init(rngs=subkeys[1], x=dummy_input)\n",
        "\n",
        "# Test whether everything works, \n",
        "# mutable=False ensures that batch_norm statistics aren't updated\n",
        "output = base_model.apply(base_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ],
      "metadata": {
        "id": "q0J0kysj6cP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd4326c-6a12-4379-9a2e-8e01959da878"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 2), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pm4n9NcYjzcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b663a056-47a7-4469-c744-d501350da4a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 128), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Source: https://www.kaggle.com/code/roguekk007/flax-model-surgery\n",
        "\n",
        "class EncoderWithProjectionHead(nn.Module):\n",
        "    encoder : nn.Sequential\n",
        "\n",
        "    def setup(self):\n",
        "        self.proj_head = nn.Sequential([\n",
        "            nn.Dense(features=128), \n",
        "            nn.relu, \n",
        "            nn.Dense(features=128)\n",
        "            ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.proj_head(x)\n",
        "\n",
        "# Use all of ResNet, except for the last layer\n",
        "encoder = nn.Sequential(base_model.layers[:-1])\n",
        "encoder_with_proj_head = EncoderWithProjectionHead(encoder=encoder)\n",
        "\n",
        "random_key_2, subkey = jax.random.split(random_key_1)\n",
        "pretrain_params = encoder_with_proj_head.init(subkey, dummy_input)\n",
        "\n",
        "# Overwrite the randomly initialized parameters with the ones we got from pretraining\n",
        "pretrain_params = unfreeze(pretrain_params)\n",
        "pretrain_params['params']['backbone'] = base_params[\"params\"] \n",
        "pretrain_params['batch_stats']['backbone'] = base_params[\"batch_stats\"]\n",
        "pretrain_params = freeze(pretrain_params)\n",
        "\n",
        "# Test whether everything works\n",
        "output = encoder_with_proj_head.apply(pretrain_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPJiOE1joICF"
      },
      "source": [
        "## Train the model using your loss function\n",
        "\n",
        "Let's see what your loss function can do in practice! This should not take more than 2 minutes per epoch. If it does, make sure you are using a GPU environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dnS_hX7l2Kfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35952273-c40e-48d1-e8bb-0fb36f9adb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:45<00:00,  2.36it/s, train_loss=0.331]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: pretrain loss 3.049849033355713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [01:54<00:00,  3.41it/s, train_loss=0.165]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: pretrain loss 0.3227316737174988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [01:55<00:00,  3.40it/s, train_loss=0.0827]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: pretrain loss 0.1672963947057724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [01:55<00:00,  3.38it/s, train_loss=0.0312]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: pretrain loss 0.11962350457906723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [01:55<00:00,  3.40it/s, train_loss=0.175]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: pretrain loss 0.09647758305072784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config = {\"learning_rate\": 0.01, \"momentum\": 0.9}\n",
        "pretrain_dataloader = NumpyLoader(pretrain_dataset, batch_size=128)\n",
        "\n",
        "pretrain_state = create_train_state(encoder_with_proj_head, pretrain_params, config)\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "  pretrain_state, pretrain_loss = pretrain_epoch(pretrain_state, \n",
        "                                                 pretrain_dataloader)\n",
        "  print(f\"Epoch {epoch}: pretrain loss {pretrain_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyXN8S4GgZcY"
      },
      "source": [
        "Hopefully you saw the loss going down! This is contrastive learning in practice. Simply by comparing positive and negative samples, the model gets trained. Of course, we still need to evaluate how useful the learned features are.\n",
        "\n",
        " But before that: Why do we even bother with negative pairs? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermezzo: Focusing on the positive\n",
        "\n",
        "\n",
        "**Task** \n",
        "1. What would we need to change in our loss function to only use positive pairs? \n",
        "2. What do you think the effect would be on training?\n",
        "\n",
        "Please answer this question before running the code below, which will give you the answer. You don't need to guess right, but you are expected to think about it and make an educated guess based on your understanding. "
      ],
      "metadata": {
        "id": "RMlom8ZV1bGi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGrJd_drhyvO"
      },
      "source": [
        "**Answer**\n",
        "* If we consider only positive pairs, we can simply use the cosine similarity and maximise it, in other words, the loss would be the negative of cosine similarities of positive pairs\n",
        "* The model would end up giving similar embeddings to all inputs to maximise cosine similarity since there is no incentive to give different embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** Quickly write the loss function that only uses positive pairs, then run the code below to see whether your prediction was right."
      ],
      "metadata": {
        "id": "4FFYMjWp-PFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def PositivesOnlyLoss(Z1, Z2):\n",
        "  L = -jnp.sum(cosine_similarity(Z1,Z2))/4\n",
        "  return L\n",
        "\n",
        "# Verify your solution\n",
        "your_output = PositivesOnlyLoss(Z1, Z2)\n",
        "desired_output = -0.06671171\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ],
      "metadata": {
        "id": "MrZiHNgf9gk8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dQxYmS1i9Xg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfd11ea-e018-4646-f413-9ee3ae702896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:24<00:00,  2.71it/s, train_loss=-20]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: -31.91294288635254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train model using only positive pairs\n",
        "\n",
        "# Note that we can use the same model and parameters as before, since these were\n",
        "# not changed by the previous training. The updated parameters are stuck in \n",
        "# pretrain_state for now. This is part of the functional character of JAX.\n",
        "positives_only_state = create_train_state(encoder_with_proj_head, \n",
        "                                          pretrain_params, config)\n",
        "\n",
        "for epoch in range(1):\n",
        "\n",
        "  positives_only_state, pretrain_loss = pretrain_epoch(positives_only_state, \n",
        "                                                       pretrain_dataloader, \n",
        "                                                       positives_only=True)\n",
        "  print(f\"Epoch {epoch}: {jnp.mean(pretrain_loss)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i1oN_gQhsCo"
      },
      "source": [
        "**Task** What did you observe? Why did this happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNAzVkBqhwGu"
      },
      "source": [
        "**Answer**\n",
        "The Model has a low and a constant loss which does not reduce further. It might be giving the same embedding irrespective of the input to achieve this loss. Therefore, positiveonly loss as defined above would not be the best metric."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are actually some more recent methods that *do* manage to use only positive pairs, without encoutering the problem you saw happen above. If you're interested how, look at one of these: [BYOL](http://arxiv.org/abs/2006.07733), [Barlow Twins](https://arxiv.org/abs/2103.03230), [SimSiam](https://arxiv.org/abs/2011.10566)"
      ],
      "metadata": {
        "id": "qGeyeeUD7RPc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyDYu7gWdPfI"
      },
      "source": [
        "## Linear evaluation - How good are the features we learned?\n",
        "\n",
        "To judge how good the features are that we learned, we want to test how useful they are for performing the task we actually care about. For this, we use the **linear evaluation** protocol. We take the trained model, discard the projection head and freeze all the model weights. We then add a single linear layer to perform our prediction task and evaluate its performance.\n",
        "\n",
        "In JAX, freezing the weights is currently done by setting the gradients to zero. We have implemented that as part of create_train_state, so you will not see it in the code below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://www.kaggle.com/code/roguekk007/flax-model-surgery\n",
        "\n",
        "class AddClassificationLayerToBackbone(nn.Module):\n",
        "    backbone : nn.Sequential\n",
        "    num_classes : int\n",
        "        \n",
        "    def setup(self):\n",
        "        self.head = nn.Dense(self.num_classes)\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return self.head(x)\n",
        "\n",
        "backbone = encoder_with_proj_head.encoder\n",
        "finetune_model = AddClassificationLayerToBackbone(backbone=backbone, \n",
        "                                                  num_classes=10)\n",
        "\n",
        "random_key_3, subkey = jax.random.split(random_key_2)\n",
        "finetune_params = finetune_model.init(subkey, dummy_input)\n",
        "\n",
        "# Note: This unfreeze has nothing to do with the notion of freezing parameters.\n",
        "# It only allows us to write into an otherwise write-protected FrozenDict.\n",
        "finetune_params = unfreeze(finetune_params)\n",
        "\n",
        "# Overwrite the randomly initialized parameters with the ones we got from pretraining\n",
        "finetune_params['params']['backbone'] = pretrain_state.params[\"encoder\"] \n",
        "finetune_params['batch_stats']['backbone'] = pretrain_state.batch_stats[\"encoder\"]\n",
        "finetune_params = freeze(finetune_params)\n",
        "\n",
        "# Test whether everything works\n",
        "output = finetune_model.apply(finetune_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ],
      "metadata": {
        "id": "O2LP40Mu52LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f2aa89-f1d4-497c-ff97-ebd7d21b04b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 10), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune the model's last layer"
      ],
      "metadata": {
        "id": "DXN3KSdj-oZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_train_dataloader = NumpyLoader(supervised_dataset_1p, batch_size=128)\n",
        "val_dataloader = NumpyLoader(val_dataset, batch_size=128)\n",
        "\n",
        "finetune_config = {\"learning_rate\": 0.01, \"momentum\": 0.9}\n",
        "finetune_state = create_train_state(finetune_model, finetune_params, finetune_config, freeze_encoder=True)\n",
        "\n",
        "for epoch in range(5):\n",
        "  finetune_state, train_loss, train_acc = supervised_epoch(state=finetune_state, train_dl=supervised_train_dataloader)\n",
        "  print(f\"Epoch {epoch}: train loss {train_loss:.2f}, train accuracy {train_acc:.2f}\")\n",
        "\n",
        "  val_loss, val_acc = compute_validation_performance(finetune_state, val_dataloader)\n",
        "  print(f\"Epoch {epoch}: val loss: {val_loss:.2f}, val accuracy: {val_acc:.2f}\\n\")"
      ],
      "metadata": {
        "id": "fS57etKU_XGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c57011-9886-4620-8310-b9b1ca171b8f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:11<00:00,  2.90s/it, train_loss=2.39, train_acc=0.112068966]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss 2.44, train accuracy 0.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:04<00:00, 18.08it/s, val_loss=2.29, val_acc=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: val loss: 2.33, val accuracy: 0.15\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.53it/s, train_loss=2.28, train_acc=0.1724138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss 2.29, train accuracy 0.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 55.06it/s, val_loss=2.3, val_acc=0.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val loss: 2.23, val accuracy: 0.19\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.78it/s, train_loss=2.12, train_acc=0.2672414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train loss 2.14, train accuracy 0.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 53.27it/s, val_loss=2.3, val_acc=0.188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val loss: 2.16, val accuracy: 0.22\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.55it/s, train_loss=1.98, train_acc=0.3275862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train loss 2.02, train accuracy 0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 52.99it/s, val_loss=2.31, val_acc=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val loss: 2.12, val accuracy: 0.23\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.72it/s, train_loss=1.94, train_acc=0.3448276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train loss 1.95, train accuracy 0.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 53.39it/s, val_loss=2.34, val_acc=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: val loss: 2.12, val accuracy: 0.24\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison to other methods\n",
        "\n",
        "How well did our self-supervised training work? Below, you can see some values for comparison. All of them have only been run for a comparatively short time. The value in brackets indicates how many labeled images have been used in supervised training or supervised finetuning.\n",
        "- Linear evaluation from random initialization and from an ImageNet-pretrained model only train the final linear layer of the network for 5 epochs.\n",
        "- Fully supervised training starts from a randomly initialized network and trains the whole network for 5 epochs.\n",
        "- SimCLR trains for 20 epochs on the full unlabeled training dataset. Then, the final linear layer is trained for 5 epochs.\n",
        "\n",
        "| Method                                 \t| Test set accuracy \t|\n",
        "|----------------------------------------\t|-------------------\t|\n",
        "| Linear evaluation from random initialization (100% labels)         \t|     0.22              \t|\n",
        "| Linear evaluation from ImageNet pretraining (1% labels) |                    0.34                 \t|\n",
        "| Linear evaluation from ImageNet pretraining (100% labels) |                   0.41                 \t|\n",
        "| Full supervised training (1% labels)   \t|   0.33                \t|\n",
        "| Full supervised training (100% labels) \t|   0.57                \t|\n",
        "| SimCLR training (1% labels)   \t|    0.25               \t|\n",
        "| SimCLR training (100% labels)   \t|  0.33                 \t|"
      ],
      "metadata": {
        "id": "cmGce4T4BWZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-supervised learning: Great if you have few labels\n",
        "\n",
        "Using self-supervision, we can make use of unlabeled data that would be useless to supervised approaches. This way, we can often achieve much better performance than pure supervised learning would achieve, especially if the labeled dataset is comparatively small. Depending on the setting, self-supervised approaches can even perform better than pure supervised approaches without using any additional unlabeled data.\n",
        "\n",
        "**Task** In this exercise, the self-supervised model did not work as well as promised in the paper. Why is that? If you had access to any ressources you need, how would you change the setup to increase the performance and hopefully beat supervised training?"
      ],
      "metadata": {
        "id": "xfP8qK4tqf2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "The model is trained only for 5 epochs. If we had access to more resources(i.e. GPU), introducing more epochs could hopefully increase the performance."
      ],
      "metadata": {
        "id": "yuQ-t5o4z_MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** How long did it take you to complete this practical? This information is valuable to us to balance the difficulty of different practicals."
      ],
      "metadata": {
        "id": "6IWWfgplJimF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "Around 5 hours"
      ],
      "metadata": {
        "id": "cjrFKhgGJlhm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHSIxSRCeGzh"
      },
      "source": [
        "# References\n",
        "[1] [Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.](https://proceedings.mlr.press/v119/chen20j.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fh7-aNi82_uo"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tMhB8RRhhZQN",
        "_jDu2uOaEspK",
        "YEFWWqTfrunL",
        "B0zXALj9rzNs",
        "lVjt4SNyrz8H",
        "VF3QqWiQkvk8"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50962a8b54ca495d83f43e7a7a04740e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5a3acc5bd114f25bf7ca64c63465753",
              "IPY_MODEL_2667369bc5a04a7e945480ed3ca59cd0",
              "IPY_MODEL_eeb7f1832ffd46789259bf1866321d95"
            ],
            "layout": "IPY_MODEL_7656b60c0acd4b5aa1160341f4d3771d"
          }
        },
        "e5a3acc5bd114f25bf7ca64c63465753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44549b87ef3443fb9da4606c1255f207",
            "placeholder": "​",
            "style": "IPY_MODEL_2341c54e12c2436f9c17fc4364126737",
            "value": "100%"
          }
        },
        "2667369bc5a04a7e945480ed3ca59cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_983cf5f62c1144eca6bc26a9b7c79cb7",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0c51f31ed1147ecb55de0b17c89f1dd",
            "value": 170498071
          }
        },
        "eeb7f1832ffd46789259bf1866321d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d898951063304256977589104be5a383",
            "placeholder": "​",
            "style": "IPY_MODEL_9956ee3e7e9c40559a3e6732b59138ee",
            "value": " 170498071/170498071 [00:05&lt;00:00, 31125901.16it/s]"
          }
        },
        "7656b60c0acd4b5aa1160341f4d3771d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44549b87ef3443fb9da4606c1255f207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2341c54e12c2436f9c17fc4364126737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "983cf5f62c1144eca6bc26a9b7c79cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c51f31ed1147ecb55de0b17c89f1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d898951063304256977589104be5a383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9956ee3e7e9c40559a3e6732b59138ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}